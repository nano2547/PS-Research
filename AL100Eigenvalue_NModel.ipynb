{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nano2547/PS-Research/blob/main/AL100Eigenvalue_NModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "toxic-delicious",
      "metadata": {
        "id": "toxic-delicious"
      },
      "outputs": [],
      "source": [
        "# https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/iet-gtd.2019.1790\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import t\n",
        "\n",
        "from AL_methods import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adj_list = {112: [72, 61, 90],\n",
        " 113: [73, 62, 91],\n",
        " 114: [74, 63, 92],\n",
        " 115: [75, 64, 93],\n",
        " 119: [76, 57, 97],\n",
        " 120: [77, 58, 98],\n",
        " 121: [78, 59, 99],\n",
        " 122: [79, 60, 100],\n",
        " 105: [65, 50],\n",
        " 106: [66, 51],\n",
        " 107: [67, 52],\n",
        " 108: [68, 53],\n",
        " 76: [119, 57, 50, 97],\n",
        " 77: [120, 58, 51, 98],\n",
        " 78: [121, 59, 52, 99],\n",
        " 79: [122, 60, 53, 100],\n",
        " 72: [112, 65, 61, 90],\n",
        " 73: [113, 66, 62, 91],\n",
        " 74: [114, 67, 63, 92],\n",
        " 75: [115, 68, 64, 93],\n",
        " 65: [105, 72, 50],\n",
        " 66: [106, 73, 51],\n",
        " 67: [107, 74, 52],\n",
        " 68: [108, 75, 53],\n",
        " 57: [119, 76, 61, 97],\n",
        " 58: [120, 77, 62, 98],\n",
        " 59: [121, 78, 63, 99],\n",
        " 60: [122, 79, 64, 100],\n",
        " 50: [105, 76, 65],\n",
        " 51: [106, 77, 66],\n",
        " 52: [107, 78, 67],\n",
        " 53: [108, 79, 68],\n",
        " 61: [112, 72, 57, 90],\n",
        " 62: [113, 73, 58, 91],\n",
        " 63: [114, 74, 59, 92],\n",
        " 64: [115, 75, 60, 93],\n",
        " 97: [119, 76, 57, 101],\n",
        " 98: [120, 77, 58, 102],\n",
        " 99: [121, 78, 59, 103],\n",
        " 100: [122, 79, 60, 104],\n",
        " 90: [112, 72, 61, 83],\n",
        " 91: [113, 73, 62, 84],\n",
        " 92: [114, 74, 63, 85],\n",
        " 93: [115, 75, 64, 86],\n",
        " 101: [97, 1, 8, 15, 22],\n",
        " 102: [98, 2, 9, 16, 23],\n",
        " 103: [99, 3, 10, 17, 24],\n",
        " 104: [100, 4, 11, 18, 25],\n",
        " 83: [90],\n",
        " 84: [91],\n",
        " 85: [92],\n",
        " 86: [93],\n",
        " 1: [101],\n",
        " 2: [102],\n",
        " 3: [103],\n",
        " 4: [104],\n",
        " 8: [101, 29, 36],\n",
        " 9: [102, 30, 37],\n",
        " 10: [103, 31, 38],\n",
        " 11: [104, 32, 39],\n",
        " 29: [8, 36],\n",
        " 30: [9, 37],\n",
        " 31: [10, 38],\n",
        " 32: [11, 39],\n",
        " 36: [8, 29],\n",
        " 37: [9, 30],\n",
        " 38: [10, 31],\n",
        " 39: [11, 32],\n",
        " 15: [101, 43],\n",
        " 16: [102, 44],\n",
        " 17: [103, 45],\n",
        " 18: [104, 46],\n",
        " 43: [15],\n",
        " 44: [16],\n",
        " 45: [17],\n",
        " 46: [18],\n",
        " 22: [101],\n",
        " 23: [102],\n",
        " 24: [103],\n",
        " 25: [104],\n",
        " 116: [],\n",
        " 117: [],\n",
        " 118: [],\n",
        " 123: [],\n",
        " 124: [],\n",
        " 125: [],\n",
        " 109: [],\n",
        " 110: [],\n",
        " 111: [],\n",
        " 80: [76, 79, 74, 66],\n",
        " 81: [77, 72, 75, 67],\n",
        " 82: [78, 73, 65, 68],\n",
        " 69: [106, 79, 74, 66],\n",
        " 70: [107, 72, 75, 67],\n",
        " 71: [108, 73, 65, 68],\n",
        " 54: [106, 76, 74, 66],\n",
        " 55: [107, 77, 75, 67],\n",
        " 56: [108, 78, 65, 68],\n",
        " 94: [106, 76, 79, 57, 60, 97, 92],\n",
        " 95: [107, 77, 72, 58, 50, 98, 93],\n",
        " 96: [108, 78, 73, 59, 51, 99, 101],\n",
        " 87: [106, 76, 79],\n",
        " 88: [107, 77, 72],\n",
        " 89: [108, 78, 73],\n",
        " 5: [74],\n",
        " 6: [75],\n",
        " 7: [65],\n",
        " 12: [74, 52],\n",
        " 13: [75, 53],\n",
        " 14: [65, 61],\n",
        " 33: [60, 62],\n",
        " 34: [50, 63],\n",
        " 35: [51, 64],\n",
        " 40: [52],\n",
        " 41: [53],\n",
        " 42: [61],\n",
        " 19: [74, 100],\n",
        " 20: [75, 90],\n",
        " 21: [65, 91],\n",
        " 47: [97],\n",
        " 48: [98],\n",
        " 49: [99],\n",
        " 26: [74],\n",
        " 27: [75],\n",
        " 28: [65]}"
      ],
      "metadata": {
        "id": "AozIaQi4XqYv"
      },
      "id": "AozIaQi4XqYv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attack_dict = {61: (2.20, 3.50, 'CT', 'C_CTW10_A'),\n",
        "               62: (2.20, 3.50, 'CT', 'C_CTW10_B'),\n",
        "               63: (2.20, 3.50, 'CT', 'C_CTW10_C'),\n",
        "               64: (2.20, 3.50, 'CT', 'C_CTW10_N'),\n",
        "               52: (3.8, 4.5, 'Cphase', 'C_CTW9_C'),\n",
        "               94: (4.5, 5.9, 'PT', 'V_PTW4_A'),\n",
        "               95: (4.5, 5.9, 'PT', 'V_PTW4_B'),\n",
        "               96: (4.5, 5.9, 'PT', 'V_PTW4_C'),\n",
        "               88: (6.2, 7.1, 'Vphase', 'V_PTW4_BN'),\n",
        "               76: (8.5, 9.4, 'GPS', 'C_CTWE1&V_PTWE1'),\n",
        "               77: (8.5, 9.4, 'GPS', 'C_CTWE1&V_PTWE1'),\n",
        "               78: (8.5, 9.4, 'GPS', 'C_CTWE1&V_PTWE1'),\n",
        "               79: (8.5, 9.4, 'GPS', 'C_CTWE1&V_PTWE1'),\n",
        "               80: (8.5, 9.4, 'GPS', 'C_CTWE1&V_PTWE1'),\n",
        "               81: (8.5, 9.4, 'GPS', 'C_CTWE1&V_PTWE1'),\n",
        "               82: (8.5, 9.4, 'GPS', 'C_CTWE1&V_PTWE1'),\n",
        "               123: (10.5, 11.5, 'PT', 'V_PT_SB_A'),\n",
        "               124: (10.5, 11.5, 'PT', 'V_PT_SB_B'),\n",
        "               125: (10.5, 11.5, 'PT', 'V_PT_SB_C'),\n",
        "               29: (11.7, 12.7, 'CT', 'C_CTFDR2R1'),\n",
        "               30: (11.7, 12.7, 'CT', 'C_CTFDR2R1'),\n",
        "               31: (11.7, 12.7, 'CT', 'C_CTFDR2R1'),\n",
        "               32: (11.7, 12.7, 'CT', 'C_CTFDR2R1'),\n",
        "               38: (14.6, 15.8, 'Cphase', 'C_CTFDR2R2_C'),\n",
        "               15: (16.5, 17.5, 'GPS', 'C_CTFDR3&V_PTFDR3'),\n",
        "               16: (16.5, 17.5, 'GPS', 'C_CTFDR3&V_PTFDR3'),\n",
        "               17: (16.5, 17.5, 'GPS', 'C_CTFDR3&V_PTFDR3'),\n",
        "               18: (16.5, 17.5, 'GPS', 'C_CTFDR3&V_PTFDR3'),\n",
        "               19: (16.5, 17.5, 'GPS', 'C_CTFDR3&V_PTFDR3'),\n",
        "               20: (16.5, 17.5, 'GPS', 'C_CTFDR3&V_PTFDR3'),\n",
        "               21: (16.5, 17.5, 'GPS', 'C_CTFDR3&V_PTFDR3')\n",
        "               }"
      ],
      "metadata": {
        "id": "f4EgCJsW01eV"
      },
      "id": "f4EgCJsW01eV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "khLgdnrukwz9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08f852e4-e6ad-4dd6-85f3-e70de88f4227"
      },
      "id": "khLgdnrukwz9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "independent-tanzania",
      "metadata": {
        "id": "independent-tanzania"
      },
      "outputs": [],
      "source": [
        "path = \"../Data/Data_Power/\"\n",
        "train_file = \"/content/drive/MyDrive/Event 4 without attacks.CSV\"\n",
        "test_file = \"/content/drive/MyDrive/Event 4N.CSV\"\n",
        "\n",
        "# df_train = pd.read_csv(train_file, skiprows=[0,1,2,4])\n",
        "df_test = pd.read_csv(test_file, skiprows=[0,1,2,4])\n",
        "# df_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "higher-pollution",
      "metadata": {
        "id": "higher-pollution"
      },
      "outputs": [],
      "source": [
        "# data_train = preprocess(df_train)\n",
        "# data_train\n",
        "data_test = preprocess(df_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "industrial-circulation",
      "metadata": {
        "id": "industrial-circulation"
      },
      "outputs": [],
      "source": [
        "alpha = 0.05\n",
        "num_sensors = data_test.shape[-1]\n",
        "t_alpha_over_2 = t.ppf(1-(alpha/2), df=num_sensors-1)\n",
        "threshold = t_alpha_over_2 / np.sqrt(num_sensors)\n",
        "\n",
        "\n",
        "def get_eigs(df):\n",
        "\n",
        "    return np.linalg.eig(df.corr())\n",
        "\n",
        "\n",
        "def threshold_eigs(eig_vals, threshold=1e-6):\n",
        "\n",
        "     return  np.abs(eig_vals) > threshold\n",
        "\n",
        "\n",
        "def anomaly_localisation(eig_values, eig_vectors, indices, alpha=0.05, hyp='t'):\n",
        "    '''\n",
        "    returns a boolean array with size of num_sensors of whether sensor is anomalous\n",
        "    '''\n",
        "\n",
        "    num_sensors = len(eig_values)\n",
        "    mag_eigs = np.abs(eig_values)\n",
        "\n",
        "    eig_vector_sq = np.square(eig_vectors)\n",
        "\n",
        "    # array of eta_i\n",
        "    eta = np.abs(eig_values[indices] @ eig_vector_sq[indices] / np.sum(mag_eigs))\n",
        "    eta_hat = (eta - np.mean(eta)) / np.std(eta)\n",
        "\n",
        "    if hyp is None:\n",
        "\n",
        "        return eta_hat\n",
        "\n",
        "    # check if within confidence interval\n",
        "    return np.logical_not(hypothesis_testing(eta_hat, left_threshold=-threshold,\n",
        "                              right_threshold=threshold, alpha=alpha, hyp=hyp))\n",
        "\n",
        "\n",
        "def hypothesis_testing(vec, left_threshold=None, right_threshold=None, alpha=0.05, hyp='t'):\n",
        "    '''\n",
        "    returns a boolean array with size of num_sensors of whether sensor is ok\n",
        "    '''\n",
        "\n",
        "    if hyp == 't':\n",
        "        if alpha != 0.05:\n",
        "            t_alpha_over_2 = t.ppf(1-(alpha/2), df=num_sensors-1)\n",
        "            threshold = t_alpha_over_2 / np.sqrt(len(vec))\n",
        "\n",
        "            # check if within confidence interval\n",
        "            left_threshold = -threshold\n",
        "            right_threshold = threshold\n",
        "\n",
        "    else:\n",
        "        # can use empirical distribution\n",
        "        pass\n",
        "\n",
        "    return np.logical_and(left_threshold <= vec, vec <= right_threshold)\n",
        "\n",
        "\n",
        "def localise_anomalies_rolling(df, window=350, threshold=1e-30, alpha=0.05, hyp='t'):\n",
        "\n",
        "    num_sensors = df.shape[-1]\n",
        "    corr_windowed = df.rolling(window).corr().dropna().to_numpy()\n",
        "\n",
        "    end = len(corr_windowed)//num_sensors\n",
        "    localisation = np.empty((end, num_sensors))\n",
        "\n",
        "    for i in range(0, end):\n",
        "\n",
        "        eig_values, eig_vectors = np.linalg.eig(corr_windowed[i*num_sensors:(i+1)*num_sensors])\n",
        "        indices = threshold_eigs(eig_values, threshold)\n",
        "        localisation[i] = anomaly_localisation(eig_values, eig_vectors, indices, alpha=alpha, hyp=hyp)\n",
        "\n",
        "    return localisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "certain-guidance",
      "metadata": {
        "id": "certain-guidance"
      },
      "outputs": [],
      "source": [
        "# test trial\n",
        "# localisation = np.logical_not(\n",
        "#     localise_anomalies_rolling(data_train.iloc[:, :10], window=350, threshold=1e-30, alpha=0.05, hyp='t')\n",
        "# )\n",
        "# localisation.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "statutory-comment",
      "metadata": {
        "id": "statutory-comment"
      },
      "outputs": [],
      "source": [
        "# localisation.sum(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "solar-consumption",
      "metadata": {
        "id": "solar-consumption"
      },
      "outputs": [],
      "source": [
        "# localisation = np.logical_not(\n",
        "#     localise_anomalies_rolling(data_train, window=350, threshold=1e-30, alpha=0.05, hyp='t')\n",
        "# )\n",
        "# localisation.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "incoming-victoria",
      "metadata": {
        "id": "incoming-victoria"
      },
      "outputs": [],
      "source": [
        "# localisation.sum(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "secret-signature",
      "metadata": {
        "id": "secret-signature"
      },
      "outputs": [],
      "source": [
        "# violations = np.logical_not(localisation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "surgical-found",
      "metadata": {
        "id": "surgical-found"
      },
      "outputs": [],
      "source": [
        "# violations.sum(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "light-pursuit",
      "metadata": {
        "id": "light-pursuit"
      },
      "outputs": [],
      "source": [
        "# with open(\"eig_event3_w350_violations.csv\", 'wb') as f:\n",
        "#     np.save(f, violations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "desperate-television",
      "metadata": {
        "id": "desperate-television"
      },
      "outputs": [],
      "source": [
        "# test_statistic_train_data = localise_anomalies_rolling(\n",
        "#     data_train, window=350, threshold=1e-30, alpha=0.05, hyp=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "orange-legislature",
      "metadata": {
        "id": "orange-legislature"
      },
      "outputs": [],
      "source": [
        "# with open(\"eig_event3_w350_statistic.csv\", 'wb') as f:\n",
        "#     np.save(f, test_statistic_train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caroline-prime",
      "metadata": {
        "scrolled": true,
        "id": "caroline-prime"
      },
      "outputs": [],
      "source": [
        "# eig_values, eig_vectors = get_eigs(data_train)\n",
        "# plt.hist(-np.log(np.abs(eig_values)), bins=[i for i in range(-1, 30, 1)])\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "copyrighted-marks",
      "metadata": {
        "id": "copyrighted-marks"
      },
      "source": [
        "# Test Data (with Attacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "collaborative-people",
      "metadata": {
        "id": "collaborative-people"
      },
      "outputs": [],
      "source": [
        "\n",
        "violations_test = localise_anomalies_rolling(data_test, window=350, threshold=1e-30, alpha=0.05, hyp='t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "peaceful-restoration",
      "metadata": {
        "id": "peaceful-restoration"
      },
      "outputs": [],
      "source": [
        "with open(\"eig_event4_w350_violations.csv\", 'wb') as f:\n",
        "    np.save(f, violations_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wanted-hotel",
      "metadata": {
        "id": "wanted-hotel"
      },
      "outputs": [],
      "source": [
        "violations_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "necessary-character",
      "metadata": {
        "id": "necessary-character"
      },
      "outputs": [],
      "source": [
        "print(all_attack_times)\n",
        "\n",
        "all_attack_indices = set()\n",
        "\n",
        "for start, end in all_attack_times:\n",
        "    attack_indices = df_test.index[(df_test['Time'] >= start * 1e6) & (df_test['Time'] <= end * 1e6)] - 1201\n",
        "    all_attack_indices.add((attack_indices[0], attack_indices[-1]))\n",
        "\n",
        "all_attack_indice = sorted(list(all_attack_indices))\n",
        "\n",
        "attacks_in = {}\n",
        "co = 0\n",
        "for key, value in attacks.items():\n",
        "    attacks_in[sorted(all_attack_indice)[co][0]] = value\n",
        "    co+=1\n",
        "\n",
        "all_attack_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "concrete-preference",
      "metadata": {
        "id": "concrete-preference"
      },
      "outputs": [],
      "source": [
        "total_normal = len(violations_test)\n",
        "window = 350\n",
        "w = 1\n",
        "\n",
        "for start, end in all_attack_indices:\n",
        "    total_normal -= (end - start + 1 + 2 * (window + w - 1))\n",
        "print(total_normal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "inclusive-optimum",
      "metadata": {
        "id": "inclusive-optimum"
      },
      "outputs": [],
      "source": [
        "violations = violations_test.T\n",
        "\n",
        "for persistency in [1, 3, 5, 10]:\n",
        "    print(\"Persistency:\", persistency)\n",
        "\n",
        "\n",
        "    # each row is sensor, each col is timestep\n",
        "#         violations = np.logical_or((rsum_list_corr > t_corr[i].reshape(-1, 1)), (rsum_list_cov > t_cov[i].reshape(-1, 1)))\n",
        "    ADD = detection_delay(violations.any(axis=0), sorted(list(all_attack_indices)), window, w, persistency)\n",
        "    print(\"DD:\", ADD)\n",
        "    if None in ADD:\n",
        "        dd_mean = None\n",
        "    else:\n",
        "        dd_mean = np.mean(ADD)\n",
        "    print(\"Average DD:\", dd_mean)\n",
        "    LD = localisation_delay(violations, sorted(list(all_attack_indices)), window, w, persistency)\n",
        "    print(\"LD:\", LD)\n",
        "    lds = [ld for sublist in LD for ld in sublist]\n",
        "    if None in lds:\n",
        "        ld_mean = None\n",
        "    else:\n",
        "        ld_mean = np.mean(lds)\n",
        "    print(\"Average LD:\", ld_mean)\n",
        "    fpr = FPR(violations.any(axis=0), all_attack_indices, window, w, total_normal, persistency)\n",
        "    print(\"FPR\", fpr)\n",
        "\n",
        "    fpr_l = []\n",
        "    for v in violations:\n",
        "        fpr_l.append(FPR(v, all_attack_indices, window, w, total_normal, persistency=persistency))\n",
        "    print(\"FPR Localisation (Mean):\", np.mean(fpr_l))\n",
        "    print(\"####################################################\")\n",
        "    print(\"FPR Localisation:\", fpr_l)\n",
        "    print(\"****************************************************\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daily-weekly",
      "metadata": {
        "id": "daily-weekly"
      },
      "outputs": [],
      "source": [
        "n = 0\n",
        "flr = []\n",
        "attack_pred = violations.nonzero()\n",
        "\n",
        "for i, (start, end) in enumerate(sorted(all_attack_indice)):\n",
        "    print(\"***********************************************************************************************\")\n",
        "    print(f'For the {order_attack[n]} attack starting at index: {start} and ending at index: {end}')\n",
        "    localisation_idx = []\n",
        "#         print(start, end)\n",
        "    start_w = start - window - w + 1\n",
        "    end_w = end + window + w - 1\n",
        "    # get all indices that are after the start of attack\n",
        "    indices = np.logical_and((attack_pred[1] >= start_w), (attack_pred[1] <= end_w))\n",
        "    #         print(idx.sum())\n",
        "    #         print(attack_pred.shape())\n",
        "    attack_window = attack_pred[1][indices]\n",
        "    attack_sensors = attack_pred[0][indices]\n",
        "\n",
        "    sensor_list = attacks_shifted_index[attack_start_times[i]]\n",
        "    sensors_attacked = [s for sensor_l in sensor_list for s in sensor_l]\n",
        "\n",
        "    false_localised_sensors = 0\n",
        "    false_localised_sensors_list = []\n",
        "\n",
        "    for sensor in np.unique(attack_sensors):\n",
        "        if sensor not in sensors_attacked:\n",
        "            sensor_indices = (attack_sensors == sensor)\n",
        "            sensor_attack_window = np.sort(attack_window[sensor_indices])\n",
        "\n",
        "            # check for persistency\n",
        "            idx = persistency_check(sensor_attack_window, persistency)\n",
        "            if idx is not None:\n",
        "                false_localised_sensors += 1\n",
        "                false_localised_sensors_list.append(sensor)\n",
        "\n",
        "    print(f'1 Hop: {sensors_attacked}')\n",
        "    print(f'False Localisations: {false_localised_sensors_list}')\n",
        "\n",
        "    flr.append(false_localised_sensors/len(sensors_attacked))\n",
        "    n += 1\n",
        "flr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "previous-digest",
      "metadata": {
        "id": "previous-digest"
      },
      "outputs": [],
      "source": [
        "np.mean(flr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "superior-tolerance",
      "metadata": {
        "id": "superior-tolerance"
      },
      "outputs": [],
      "source": [
        "test_statistic_test_data = localise_anomalies_rolling(\n",
        "    data_test, window=350, threshold=1e-30, alpha=0.05, hyp=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "speaking-mortality",
      "metadata": {
        "id": "speaking-mortality"
      },
      "outputs": [],
      "source": [
        "with open(\"eig_event4_w350_statistic.csv\", 'wb') as f:\n",
        "    np.save(f, test_statistic_test_data)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}